
# ECE695 Modern Data Center Systems

*Guangtong Shen*

*Fall 2016*

##  17. Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing  

### Summary 
The paper introduces RDD and spark. RDDs are objects storing operations forming the lineage graph of computation flow.
Spark runs action on RDDs by examining the RDDs' lineage graph to build a DAG of stages to execute. The orginal HDFS blocks and intermediate results are partitions. Partitions in memory will be used in each stage and non-in memory partitions are computed using former partitions in the DAG.
RDD is fault tolerent becausea lost partition in a failed machine can be rebuild using the RDD defining the operation. 

### Q1: Spark gained a lot of attention and usage in the last few years. Compared to MapReduce and Dryad, what technical advantages does the Spark system have? What about broader reasons behind its success?

1. Spark is able to perform flexible computing flow like Dryad. It takes full use of in memroy partitions without serializing partitions to persistent blocks on disk like MapReduce so the parallel computing speed is several tens times MapReduce. 

2. I think the broader reasons are the benifits of Spark on Data Analitics and Machine Learning. These two become popular big data tasks and Spark becomes the most efficient platform.

### Q2: How does Spark handle failures and stragglers?
Fault-Tolerance:
An RDD logs the transformation used to build a dataset rather than the actual data. If a partition of the RDD is lost, the RDD has enough information about how it was derived from other RDDs to recompute just that partition.

Stragglers:
Same as MapReduce, when most tasks are finished, the master schedules backup executions of the remaining in-progress tasks. The backup executions may finished before the original ones so that straggler problems on bad machines are resolved.



### Q3: MapReduce was implemented in C++. Hadoop (open source version of MapReduce) was implemented in Java. Spark was implemented in Scala. Why do you think they made the decisions to use these languages? (you may need to do a little bit Google search to answer this)

It depends on the maturity of the language when the project starts. 
Google developed MapReduce before Hadoop. It used C++ mainly because it was powerful in low level system programming and was widely used in 2004. Hadoop chose Java in 2005 becasue it has lots of libraries to use and less prone to crashing. Recently modern languages have more convinient features than C++ and Java with still high performance on modern machines. In 2009, Spark chose Scala, a functional programming languange mostly because of its combination of
conciseness (which is convenient for interactive use) and efÔ¨Åciency (due to static typing).