# ECE695 Modern Data Center Systems

*Guangtong Shen*

*Fall 2016*

## 30. TensorFlow: A system for large-scale machine learning (OSDI'16) 

#### Summary

TensorFlow is another machine learning engine that train models in a single graph data flow distributed on clusters with multicore CPU/GPU/TPU and also support 
inference on various platforms even mobile devices. Vertices represent computation with mutable states, unifying computation with state management. No Parameter Server, only ‘PS’ tasks. It achieves good results with synchronous replication.


#### Q1: Compare to Project Adam, what do you think are the major advantages of TensorFlow? Name at least two.

1. Flexibility
Primitive Operators, Support advanced optimization without touching the system implementation

2. Automatic differentiation – adds the subgraph for computing gradient

3. No parameter server, but PS tasks. 



#### Q2: How does TensorFlow solve sparse data representation problem?
Either encode the data into variable-length string elements of a dense tensor, or use a tuple of dense tensors 



#### Q3: Contrary to Project Adam, TensorFlow supports synchronous execution. Do you think synchronous execution is a good idea? Can synchronous execution reduces convergence time?

Synchronous training may be faster.
In the asynchronous case (Figure 5(a)), each worker reads the current values of parameters when each step begins, and applies its gradient to the (possibly different) current values at the end: this approach ensures high utilization, but the individual steps use stale parameter values, making each step less effective.
Synchronous training needs to solve stragger problem.

