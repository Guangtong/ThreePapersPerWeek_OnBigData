# MapReduce: Simplified Data Processing on Large Clusters
### Guangtong Shen

## Summary
Map-Reduce is a parallel computing framework working on GFS. Users specify a map function that processes a
key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. GFS data nodes will be responsible for map of data on its self and send mapped data to reducer (other data nodes) to do reduce and store result.

## Q1: Do you think this abstraction is enough to express and implement most datacenter applications? Can you give an example that is difficult to write with MapReduce?

I think map-reduce is very suitable for big data parallel computing in most data center applications. It is highly extendable, fault-tolerant.
The problem is single node low efficiency and data transfer between mapper and reducer. So it is not suitable for real time applications and iterative calculation, since disk IO and data transfer cost  large proportion of time.
Also 

## Q2: MapReduce is probably the most widely adopted idea from a systems paper in the past decade. Why do you think so?

1. The growing need of ETL(Extraction, Transform, Load) of big data and data mining requires such parallel framework.
2. It simplifies application design mode to only map and reduce.
3. Hadoop ecosystem is open-source and widely used.

## Q3: What is "straggler"? Why does straggler happen? How does MapReduce solve the straggler problem?

“straggler”: a machine that takes an unusually long time to complete one of the last few map or reduce tasks in the computation, due to bad disk or high load on some machines.

The solution is when most tasks finished,  the master schedules backup executions of the remaining in-progress tasks. The backup execution may finished first so that straggler problem on bad machines are resolved.
