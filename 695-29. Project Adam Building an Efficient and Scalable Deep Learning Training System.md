# ECE695 Modern Data Center Systems

*Guangtong Shen*

*Fall 2016*

## 29. Project Adam Building an Efficient and Scalable Deep Learning Training System  Wang

#### Summary

Adam is a scalable deep learning training system built from commodity PCs. It achieves scalability and efficiency via whole-system co-design and exploiting asynchrony.

#### Q1 Deep learning is THE hottest topic in machine learning area. There are quite a few specialized systems for deep learning like Adam. Many of the techniques used in Adam are not new and have been used quite extensively in distributed systems. So do you think we can use a generic framework system to represent DNN problems and efficiently run them? What about the graph systems we've read about? What about Hadoop Spark?

Yes. There are general frameworks for machine learning and deep learning tasks, mahout on hadoop and mllib on spark. For graph systems, there is GraphX which runs on Spark.


#### Q2 Do you think DNNs are computation-bound, memory-bound, disk-bound, or network IO bound

Computation-bound. So The computation is distributed to many machines, computing in parallel.

#### Q3 Do you think CPU or GPU is more efficient in running deep learning problems (in terms of performance and CPUGPU utilization)

GPU is more efficient since it has many Arithmetic Logic Units which are very suitable for parallel computing.
