Powergraph is a new framework for distributed graph-parallel computation on natural graphs.

We are transitioning from the idea of big data to the think of big graphs. We do this because we realized that graphs are ubiquitous, from social media to science, to advertising, and to the web. Graphs encode relationships between people, products, ideas, facts and interests.
And these graph are big, with billions of vertices and edges with rich meta data.

These graphs are also essential to data mining and machine learning. They help us to identify influential people and information, find communities, target ads and products and model complex data dependencies.

Interestingly, all these graphs share a common property. They are all examples of natural graphs, which are derived from natural phenomena. And this leads to a serious problem. Existing distributed graph computation systems perform poorly on Natural Graphs. For example, if we look at the task of page-rank on Twitter Follower graph which is a natural graph with 40 million users, and 1.4 billion links.
We can see the existing systems have made an order of magnitude improvements over things like hadoop. But by exploiting the properties of Natural Graphs, we can achieve next order of magnitude improvement.

So to do that, we need to understand the properties of natural graph. One of the defining characteristics is the power law degree distribution. So here I plot the power law degree distribution of vertices in the alta vista WebGraph in log-log scale. The first thing to notice is on the top left we have more than ten to the eight vertices which have just one neighbor. And on the bottom right, we see there is a small fraction, roughly 1 percent of the vertices, which are adjacent to 50% percent of the edges in the graph. That is we have a small set of very high degree vertices. 

This leads to a star like motif, where in the center we have the high degree vertex, on the perimeter we have a large set of low degree vertices. If you think the twitter scenario, the center of the graph corresponds to president Obama and the perimeter, the followers. 

In addition, the power-law graphs are difficult to partition. To get an intuition why, we turn to our start like motif, to evenly assign vertices to machines, would require us to have to cut a large fraction of the edges in the graph. In general, researchers show that Power-Law graphs do not have good low cost balanced cuts. And traditional graph partitioning techniques typically perform poorly on these graphs.

So in summary the power law degree distribution leads to high degree vertices, which we'll show are both computationally challenging and contribute heavily to communication and storage overhead. In addition, since these graphs are difficult to partition, making it challenging to place them in a large distributed environment.

We are going to address both these problems by transforming the graph and splitting the high degree vertices across multiple machines. We are also going to introduce a new abstraction, which allow users to program for the original graph. But then have their program run equivalently and efficiently on a transformed graph.

So how do we program a graph? The ongoing philosophy to this problem is to think like a vertex. so how do we think like a vertex. This idea is encapsulated in the classic graph parallel abstraction which says that a user writes a vertex program which runs on each vertex, and the graph constrains the interaction of vertex programs to along edges.

So in the context of the popular Pregel abstraction, vertex programs can interact by passing messages to neighboring vertices. Or in the popular Graphlab abstraction, vertex programs can interact through shared state. In either case, parallelism is achieved by run multiple vertex programs simultaneously.

To get a intuition of the kind of computation we perform on these graphs, consider the task that answering what is the popularity of this user. Her popularity depends on the popularity of her followers which in turn depends on the popularity of their followers. We can encode this relationship in the form of the classic page rank algorithm, which says the rank of user i, is simply a weighted sum of user i's neighbors. We can update these ranks in parallel and we iterate this process until convergence. Now we return to the Pregel abstraction, say how do we implement page-rank in a graph parallel abstraction, so here if we look at the vertex program running on the vertex side, the first thing we need to do , is to receive all of our messages. And the messages encode weighted values of its neighbors ranks and it's going to compute the sum. Once we computed the sum, we can update the rank. We then iterate over each of our out edges, computing a new message which contains our rank weighted by the weight of the edge and sending that to each of the neighbors. We can also implement this in the Graphlab shared memory abstraction. In a different way, the vertex program begins by directly reading the rank of the neighboring vertices and computing the weighted sum. We then use that value to compute the new rank for the center vertex. In this case, if the center vertex can not converge, we are going to iterate each of our out edges, triggering the vertex program to be rerun on our neighbors. 

So now we can return to what are the challenges of high-degree vertices in the context of these graph parallel abstractions. The most apparent challenge immediately is we do a lot of iterating on our neighborhood.  And this is done within a single vertex program and therefore sequentially on a single machine. In the case of Pregel abstraction, we are going to generate a large number of messages and in many cases these messages will be identical. In the case of Graphlab abstraction, we are going to touch a large amount of state of our neighborhood and pull those information to a single machine. This can contribute to network overhead and storage. 

In both cases, we expose adjacency structure of a single vertex to the vertex program. This includes all the neighbors and a lot of meta data. This could potentially overwhelm a single machine. The Graphlab abstraction has a really cool asynchronous locking model but unfortunately it doesn't scale well to high-degree vertices.  The Pregel abstraction model runs this on a synchronous model which has frequent barriers. These barriers are prone to stragglers which can be exaggerated by the presence of high-degree vertices. 

Let's look at the communication overhead which is the dominant issue. We' dive into the details of communication for both Pregel and Graphlab abstractions. So first look at Pregel, consider the case we have large amount of messages inbound to the high-degree vertex. Pregel introduces a clever idea, in the form of commutative associative combiners. This is a user defined function which allows to combine messages on the same machine and then send only a single message across the network. This can really bring down the effects of high-degree vertex but unfortunately it doesn't work in the opposite direction, where I change the vertex D so could generate potentially identical messages which are sent through the network potentially to the same machine. 

So we can actually character the fact by running page-rank on synthetic power-law graphs. We vary the power law constant alpha, to control the presence of high-degree vertices. We are going to user piccolo as a proxy for Pregel. So the x-axis is the power-law constant and the smaller alpha implies more high-degree vertices. If I direct all of my edges into the high-degree vertices and run this abstraction we see the presence of message combiners is able to mitigate the effect of high-degree vertices, bringing the communication to a constant. But if I reverse the edges, having them leaving the high-degree vertices, I enter the broadcast scenario where now the high-degree vertices contribute heavily to the increase of the communication of the total system. 

We can also look at how Graphlab deals with this. So Graphlab maintains a shared mem review. and to do that, it makes a local copy of the vertices called ghosts. vertex D is copied onto machine 1 as a ghost on machine 1. A change of the page rank on vertex D on machine 2 is then propagated by the system to machine 1 in the form of a single message, addressing the broadcast situation. unfortunately, Graphlab lacks the notion of message combiners, so changes to the neighbors of D, A B and C, would generate large amount of traffic. We can then look at this effect by running on the same set of power-law graphs. One thing to keep in mind is that Graphlab is going to treat edges symmetrically so we get a single curve. We will generally see that the Graphlab uses substantially more network communication than the other abstraction.  

One way to bring down and manage the communication is through graph partitioning. Graph parallel abstractions rely on partitioning to minimize communication and balance computation and storage. This is achieved by assigning vertices to machines and then cutting across edges. And the number of edges cut is proportional the the network traffic of the system.

But since power-law graphs are difficult to partition, both Graphlab and Pregel resort to random hash based partitioning, which vertices are randomly assigned on machines and edges are allowed to span machines. It's actually very simple analysis you can do to compute the expected number of edges you would cut, that is simply one minus one over the number of machines. So at 10 machines you are cutting 90% of your graph. At 100 machines you are already cutting 99% of your graph. This is a substantial matter of the graph. 

So in summary Graphlab and Pregel are not well suited for natural graphs, due to challeges of high degree vertices and low quality partitioning. 

PowerGraph is going to address these issues by introducing the GAS decomposition which is going to allow us to distribute a single vertex program across the entire cluster, moving computation to the data and parallelizing these high-degree vertices. And by doing that we are going to introduce the opportunity to use vertex partitioning which allows us to effectively distribute large power-law graphs. 

We derive this abstraction by identifying a common pattern across a wide range of machine learning and data mining vertex programs. The general pattern we observe is the program begins by computing information about its neighborhood so it gathers information about neighboring vertices and use it to update the state of the vertex and then we typically iterate over our neighbors once more, either triggering future computation or updating edges metadata. 


So we generalizes the form of the GAS decomposition, which breaks the vertex program into gather, apply, and scatter phases. In the gather phase, a user defined gather function is executed on each edge returning a partial accumulate. The user defined commutative and associative sum is used to merge these accumulates. So to execute on the vertex Y, in parallel I can construct a sum of all my neighborhood. And then into the apply phase, with user defined apply function, takes that sum, and update the center of vertex value. Then we enter the final scatter phase, where user defined scatter function is again executed in parallel on all the neighboring vertices, updating edge data and triggering future computation. 

We can  return to the page-rank example and see how do we implement page-rank in this new abstraction. We have to define a gather function which simply returns the weighted value of the neighboring vertex. Our sum function is simply the arithmetic sum. The apply function just updates the center vertex value. And the scatter function then triggers the neighboring vertices when the center vertex value has changed.

So now we have this new abstraction we can actually split or transform our graph. So we are going to take the high-degree vertex and split it across multiple machines. Then we are going to randomly select a machine to be the master for that vertex and the remaining machines will be mirrors. We can then execute the gather function entirely in parallel in a distributed way, each of these machines computing a partial sum. We can then send these partial sums to machine 1, where the final sum is computed.  We've completed the gather phase. 

Then we can execute the apply by using the user defined apply function to take that partial sum and transform the center vertex. And we replicate the transformed vertex to all the mirrors. 

Then we can enter the scatter phase again in parallel on each machine, updating edge meta data and triggering neighboring vertices to be computed. 

In the previous example, we noticed the communication corresponds to the number of machines each vertex spans. And in fact we can minimize that by constructing a vertex cut. And excitingly, percolation theory suggests that power-law graphs have good vertex cuts. 

So rather than cutting on graph along edges where we have to synchronize a large amount of meta data, we are going to propose to cutting graphs along vertices where we only synchronize a small amount of data. We have a new theorem: for any edge-cut we can directly construct a vertex-cut which requires strictly less communication and storage. So we can improve upon existing systems.

So constructing vertex-cut announce evenly assigned edges to machines in order to minimize the number of machines spanned by each vertex. In our case, we want to do it efficiently so we are going to actually assign the edges when we are constructing the partition as the graph is loaded, touching each edge only once. 

We are going to propose three distributed approaches: one random placement and two greedy placements. 

The random edge-placement is actually very simple. Given a graph, I randomly assign the edges of the graph to the machines. The first thing to notice is edge data is not replicated, it's uniquely assigned to one machine. Second thing to notice in this example is this is a balanced cut since I have assigned edges evenly to each machine. 

Now since the communication corresponds to the number of machines spanned by each vertex, the vertex Y which spans 3 machines will contribute most communication, followed by vertex Z. But the black vertices will not contribute to communication or storage overhead because they are uniquely assigned to machines. 

So we can analyze this random edge-placement algorithm. we can compute the expected number of machines spanned by a vertex as a function of the size of the cluster.  I'll explain the equation, here is what is looks like. We can execute the same edge-placement algorithm on the twitter follower graph and see we get a very close approximation to the number of ways of vertex-cut which allows us to accurately estimate memory and communication overhead over subsequent execution of the system.

We can also take the analysis of the random edge-placement algorithm to compare it with the previous analysis done on just random edge cuts which Pregel and Graphlab use. We can compute the reduction in communication and storage overhead as a function of the number of machines. We can show that for very large clusters we can still achieve order of magnitude improvement on reductions in communication and storage. 

We can then take the random edge-placement algorithm and improve upon it by greedily assigning edges to machines. We are going to place edges on machines which already have the vertices in that edge. So if I have the edge A-D, I'd like to assign it to machine one because machine one already has vertex A. If I encounter an edge that could be assigned to more than one machines, I am going to assign it to the least loaded machine to ensure the balanced objective. 

We can show this greedy cut is in fact a direct de-randomization of the random edge-placement algorithm therefore greedily minimizes the expected number of machines spanned. We propose two distributed implementations for this algorithm. One is the coordinated edge-placement, which requires each machine to coordinate on the previous history of other vertices where they've been placed. It's therefore slower but will generate higher quality cuts. We have a simple oblivious algorithm which we are going to approximate the greedy objective by have each machine maintain only a local history where vertices have been previous assigned. This will be faster but will generate lower quality cuts.

So we can then look at the performance of this on the twitter follower graph both in terms of the cost (Comm + Storage, corresponds to number of machines) of the cut this finally constructed as well as the running time it takes to construct that cut. We'll do this as a function of our cluster. So the x-axis has the number of machines. We first look at the random placement. We see the as we increase the number of machines, the cost of the cut goes up but time to construct that cut goes down because it is a fully distributed algorithm. We can now drastically improve upon that cost by using our coordinated placement objective. This is of course going to increase the time it takes to construct the cut. We have a compromise by using the oblivious placement objective which will bring the cost down with only a small increase on the time to construct the cut.

Then we can look at the effect of these partitioning algorithms on the performance of algorithms like page-rank, collaborative filtering and shortest path. So we are going to run our system on each of these algorithms and compare against the running time of just the random cut. We show the oblivious and coordinated greedy objectives can reduce the runtime of the subsequent algorithm.  

There are some features I am not going to discuss in detail. Our system supports three execution modes. One is the synchronous execution which is similar to the execution model described in Pregel. Asynchronous execution which allows us to interleave Gather, Apply, Scatter phases. An asynchronous and serializable execution which ensures neighboring vertices do not run simultaneously. 

In addition we introduce a novel Delta Caching, which accelerate gather phase by caching partial sums for each vertex.  






































